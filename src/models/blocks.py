"""
Authors: Nicolas Raymond

Description: Stores generic blocks that can be integrated into neural network architectures.
"""

from torch import abs, cat, sign, sqrt, Tensor, where
from torch.nn import AvgPool1d, Conv1d, Conv2d, BatchNorm1d, Dropout, Dropout1d, \
    Identity, LeakyReLU, Linear, MaxPool1d, Module, ModuleList, Sequential
from torch.nn.functional import softmax


class TSSR(Module):
    """
    Activation function presented in "TSSR: A Truncated and Signed Square
    Root Activation Function for Neural Networks"
    
    See https://arxiv.org/pdf/2308.04832.pdf.
    """
    def __init__(self) -> None:
        """
        Sets the original activation.
        """
        super().__init__()

    def forward(self, x: Tensor) -> Tensor:
        """
        Executes the forward pass.

        Args:
            x (Tensor): input tensor of any size.

        Returns:
            Tensor: same tensor with activation function applied to all values.
        """
        return where(abs(x) <= 1, x, sign(x)*(2*sqrt(abs(x)) - 1))

class LinearBlock(Module):
    """
    Generic Linear Block architecture.
    
    Linear -> Activation -> Dropout
    """
    def __init__(self,
                 in_features: int,
                 out_features: int,
                 dropout: float,
                 bias: bool = True,
                 use_odd_activation: bool = False) -> None:
        """
        Sets the layers of the block.

        Args:
            in_features (int): number of features in the input tensor.
            out_features (int): number of features in the output tensor.
            dropout (float): dropout probability
            bias (bool, optional): if True, bias is included in the linear layer.
                                   Default to True.
            use_odd_activation (bool, optional): if True, TSSR activation is used
                                                 instead of LeakyReLU.
                                                 Default to False.
        """
        # Call parent's constructor
        Module.__init__(self)

        # Initialize a list of layers
        self.__layers = [Linear(in_features=in_features, out_features=out_features, bias=bias)]

        # Add the proper activation function
        if use_odd_activation:
            self.__layers.append(TSSR())
        else:
            self.__layers.append(LeakyReLU())

        # Add dropout layer
        if 0 <= dropout < 1:
            self.__layers.append(Dropout(p=dropout))
        else:
            raise ValueError(f'Invalid dropout value: {dropout}. Must be in range [0, 1)')

        self.__layers = Sequential(*self.__layers)

    def forward(self, x: Tensor) -> Tensor:
        """
        Executes the forward pass.
        
        Args:
            x (Tensor): Batch of 1D data points (N, IN FEATURES)
            
        Returns:
            Tensor: Batch of 1D data points (N, OUT FEATURES)
        """
        return self.__layers(x)

class DenseBlock1D(Module):
    """
    Dense block inspired from DenseNet architecture.
    
    https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html
    """
    def __init__(self,
                 in_channels: int,
                 growth_rate: int = 24,
                 nb_conv_block: int= 6,
                 dropout: float = 0,
                 add_residual: bool = False,
                 shrink_to_growth_rate: bool = False) -> None:
        """
        Initializes the pre-bottleneck, the post-bottleneck and the convolution blocks.

        Args:
            in_channels (int): number of input channels.
            growth_rate (int): number of channels generated by each convolution block (except last).
            nb_conv_block (int): number of convolution blocks in the dense block.
            dropout (float, optional): Dropout probability in dense layers. 
                                       Default to 0.
            add_residual (bool, optional): if True, add residual connection between the input
                                           and the output of the dense block. 
                                           Default to False.
            shrink_to_growth_rate (bool, optional): if True, apply bottleneck to the input so
                                                    it has a number of channels equal to 'growth_rate'.
                                                    Default to False.
        """
        super().__init__()

        if nb_conv_block < 2:
            raise ValueError('Number of convolution blocks must be at least 2')

        # Save residual attribute
        self.__add_residual: bool = add_residual

        # Initialize pre-bottleneck
        if in_channels != growth_rate and shrink_to_growth_rate:
            self.__pre_bottleneck = CNNBlock1D(in_channels=in_channels,
                                               out_channels=growth_rate,
                                               kernel_size=1,
                                               pool_size=None,
                                               stride=1)
        else:
            self.__pre_bottleneck = Identity()

        # Initialize post-bottleneck
        if self.__add_residual:
            if growth_rate != in_channels:
                self.__post_bottleneck = CNNBlock1D(in_channels=growth_rate,
                                                    out_channels=in_channels,
                                                    kernel_size=1,
                                                    pool_size=None,
                                                    stride=1)
            else:
                self.__post_bottleneck = Identity()
        else:
            self.__post_bottleneck: Module = None

        # Initialize convolution blocks
        in_channels = growth_rate if shrink_to_growth_rate else in_channels
        self.__conv_blocks = ModuleList([CNNBlock1D(in_channels=in_channels + growth_rate*i,
                                                    out_channels=growth_rate,
                                                    kernel_size=3,
                                                    pool_size=None,
                                                    stride=1,
                                                    padding=1,
                                                    dropout=dropout)
                                         for i in range(0, nb_conv_block)])

    def forward(self, x: Tensor) -> Tensor:
        """
        Executes the forward pass.
        
        Args:
            x (Tensor): Batch of sequence (N, NB CHANNELS, LENGTH)
            
        Returns:
            Tensor: Encoded sequences (N, GROWTH RATE, LENGTH)
        """
        # Apply pre-bottleneck
        concatenated_output = self.__pre_bottleneck(x)
        
        # Apply dense layers
        for block in self.__conv_blocks:
            out = block(concatenated_output)
            concatenated_output = cat([concatenated_output, out], dim=1)

        # Apply residual connection
        if self.__add_residual:
            out = x + self.__post_bottleneck(out)

        return out

class CNNBlock1D(Module):
    """
    Core block of the CNN1D architecture.
    
    Conv1D -> BatchNorm1D -> LeakyReLU -> Dropout -> Pooling
    """
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 kernel_size: int,
                 pool_size: int = None,
                 pooling_method: str = 'max',
                 stride: int = 1,
                 padding: int = 0,
                 dropout: float = 0,
                 extended: bool = False,
                 add_residual: bool = False,
                 ) -> None:
        """
        Initializes the layers of the block.

        Args:
            in_channels (int): number of input channels.
            out_channels (int): number of output channels.
            kernel_size (int): size of convolution kernel.
            pool_size (int, optional): kernel size of pooling layer.
                                       If None, no pooling layer is included.
                                       Default to None.
            pooling_method (str, optional): pooling method ('max', 'avg' or 'attn').
                                            Default to 'max'.
            stride (int, optional): convolution stride.
                                    Default to 1.
            padding (int, optional): zero padding used during convolution. 
                                     Default to 0.
            dropout (float, optional): Dropout probability. 
                                       Default to 0.
            extended (bool optional): if True, the block is extended with additional
                                      layers in the middle. 
                                      Default to False.
        """
        # Call parent's constructor
        Module.__init__(self)

        if not 0 <= dropout < 1:
            raise ValueError(f'Invalid dropout value: {dropout}. Must be in range [0, 1)')

        if pool_size is not None and (pooling_method not in ['max', 'avg', 'attn']):
            raise ValueError(f'Invalid pooling method: {pooling_method}. \
                             Must be "max", "avg" or "attn"')

        if add_residual and not extended:
            raise ValueError('Adding a residual connection is only possible if extended is True')

        # Save residual attribute
        self.__add_residual: bool = add_residual

        # Set the base layers
        self.__base_layers = Sequential(Conv1d(in_channels=in_channels,
                                               out_channels=out_channels,
                                               kernel_size=kernel_size,
                                               stride=stride,
                                               padding=padding),
                                        BatchNorm1d(num_features=out_channels),
                                        LeakyReLU(),
                                        Dropout1d(p=dropout))

        # Add extra layers if required
        if extended:
            self.__extra_layers = Sequential(Conv1d(in_channels=out_channels,
                                                    out_channels=out_channels,
                                                    kernel_size=kernel_size,
                                                    stride=stride,
                                                    padding=padding),
                                             BatchNorm1d(num_features=out_channels),
                                             LeakyReLU(),
                                             Dropout1d(p=dropout))
        else:
            self.__extra_layers = Identity()

        # Set the pooling layer
        if pool_size is not None:
            if pooling_method == 'max':
                self.__pooling_layer = MaxPool1d(kernel_size=pool_size)
            elif pooling_method == 'avg':
                self.__pooling_layer = AvgPool1d(kernel_size=pool_size)
            else:
                self.__pooling_layer = AttentionPool1D(in_channels=out_channels,
                                                       kernel_size=pool_size)
        else:
            self.__pooling_layer = Identity()

    def forward(self, x: Tensor) -> Tensor:
        """
        Executes the forward pass.
        
        Args:
            x (Tensor): Batch of sequence (N, NB CHANNELS, LENGTH)
            
        Returns:
            Tensor: Encoded sequences (N, NB OUT CHANNELS, NEW LENGTH)
        """
        x = self.__base_layers(x)

        if self.__add_residual:
            return self.__pooling_layer(x + self.__extra_layers(x))

        return self.__pooling_layer(self.__extra_layers(x))

class AttentionPool1D(Module):
    """
    Inspired from the attention pooling layer of the Enformer architecture:
    https://www.nature.com/articles/s41592-021-01252-x
    """
    def __init__(self,
                 in_channels: int,
                 kernel_size: int = 2) -> None:
        """
        Initializes the weights of the convolution filters generating
        the unnormalized attention scores.

        Args:
            in_channels (int): number of input channels
            kernel_size (int, optional): size of the pooling window. 
                                         Default to 2.
        """
        super().__init__()

        self.__kernel_size: int = kernel_size
        self.__conv_layer = Conv2d(in_channels, 1, kernel_size=1, bias=False)

    def forward(self,  x: Tensor) -> Tensor:
        """
        Executes an attention pooling operation.
        
        Args:
            x (Tensor): Hidden states (BATCH SIZE, NB CHANNELS, SEQ LENGTH)

        Returns:
            Tensor: Pooled hidden states (BATCH SIZE, NB CHANNELS, SEQ LENGTH/POOL_SIZE)
        """

        # Reshape the tensor for the pooling operation
        # (BATCH SIZE, NB CHANNELS, SEQ LENGTH) ->
        # (BATCH SIZE, NB CHANNELS, SEQ LENGTH/POOL_SIZE, POOL_SIZE)
        x = x.reshape(x.shape[0], x.shape[1], -1, self.__kernel_size)

        # Calculate the attention
        # (BATCH SIZE, NB CHANNELS, SEQ LENGTH/POOL_SIZE, POOL_SIZE) ->
        # (BATCH SIZE, 1, SEQ LENGTH/POOL_SIZE, POOL_SIZE)
        attn = softmax(self.__conv_layer(x), dim=-1)

        # Apply attention pooling
        # (BATCH SIZE, NB CHANNELS, SEQ LENGTH/POOL_SIZE, POOL_SIZE) ->
        # (BATCH SIZE, NB CHANNELS, SEQ LENGTH/POOL_SIZE)
        return (attn*x).sum(dim=-1)
